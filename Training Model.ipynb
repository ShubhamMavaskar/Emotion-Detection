{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f915a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27edfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "val_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d927ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "'train',\n",
    "target_size=(48,48),\n",
    "batch_size=64,\n",
    "color_mode=\"grayscale\",\n",
    "class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc09b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_data_gen.flow_from_directory(\n",
    "'test',\n",
    "target_size=(48,48),\n",
    "batch_size=64,\n",
    "color_mode=\"grayscale\",\n",
    "class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa161145",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape =(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation = 'relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation = 'softmax'))\n",
    "\n",
    "emotion_model.compile(loss = 'categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6b23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-82b1ac323e24>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 555s 1s/step - loss: 1.7999 - accuracy: 0.2590 - val_loss: 1.7152 - val_accuracy: 0.3260\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 194s 433ms/step - loss: 1.6271 - accuracy: 0.3639 - val_loss: 1.5251 - val_accuracy: 0.4171\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 1.5207 - accuracy: 0.4145 - val_loss: 1.4465 - val_accuracy: 0.4470\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 263s 587ms/step - loss: 1.4480 - accuracy: 0.4448 - val_loss: 1.4018 - val_accuracy: 0.4616\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 292s 651ms/step - loss: 1.3919 - accuracy: 0.4674 - val_loss: 1.3465 - val_accuracy: 0.4902\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 157s 350ms/step - loss: 1.3454 - accuracy: 0.4887 - val_loss: 1.3054 - val_accuracy: 0.5031\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 160s 356ms/step - loss: 1.3013 - accuracy: 0.5052 - val_loss: 1.2799 - val_accuracy: 0.5077\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 1.2604 - accuracy: 0.5224 - val_loss: 1.2753 - val_accuracy: 0.5173\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 155s 346ms/step - loss: 1.2239 - accuracy: 0.5378 - val_loss: 1.2162 - val_accuracy: 0.5349\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 157s 350ms/step - loss: 1.1935 - accuracy: 0.5522 - val_loss: 1.2016 - val_accuracy: 0.5424\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 157s 350ms/step - loss: 1.1627 - accuracy: 0.5645 - val_loss: 1.1778 - val_accuracy: 0.5513\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 157s 351ms/step - loss: 1.1344 - accuracy: 0.5752 - val_loss: 1.1568 - val_accuracy: 0.5656\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 158s 353ms/step - loss: 1.1074 - accuracy: 0.5846 - val_loss: 1.1474 - val_accuracy: 0.5681\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 159s 354ms/step - loss: 1.0831 - accuracy: 0.5949 - val_loss: 1.1379 - val_accuracy: 0.5670\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 160s 357ms/step - loss: 1.0557 - accuracy: 0.6046 - val_loss: 1.1279 - val_accuracy: 0.5745\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 160s 356ms/step - loss: 1.0297 - accuracy: 0.6180 - val_loss: 1.1189 - val_accuracy: 0.5780\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 160s 358ms/step - loss: 1.0083 - accuracy: 0.6240 - val_loss: 1.1090 - val_accuracy: 0.5830\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 161s 360ms/step - loss: 0.9824 - accuracy: 0.6362 - val_loss: 1.1049 - val_accuracy: 0.5837\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 161s 359ms/step - loss: 0.9614 - accuracy: 0.6449 - val_loss: 1.0952 - val_accuracy: 0.5879\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 161s 360ms/step - loss: 0.9376 - accuracy: 0.6546 - val_loss: 1.0880 - val_accuracy: 0.5876\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 162s 361ms/step - loss: 0.9121 - accuracy: 0.6645 - val_loss: 1.0822 - val_accuracy: 0.5894\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 162s 361ms/step - loss: 0.8859 - accuracy: 0.6741 - val_loss: 1.0807 - val_accuracy: 0.5958\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 162s 361ms/step - loss: 0.8628 - accuracy: 0.6814 - val_loss: 1.0878 - val_accuracy: 0.5917\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 162s 361ms/step - loss: 0.8413 - accuracy: 0.6943 - val_loss: 1.0805 - val_accuracy: 0.5974\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 162s 361ms/step - loss: 0.8236 - accuracy: 0.7006 - val_loss: 1.0748 - val_accuracy: 0.5985\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 166s 371ms/step - loss: 0.7989 - accuracy: 0.7060 - val_loss: 1.0857 - val_accuracy: 0.6004\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 163s 365ms/step - loss: 0.7729 - accuracy: 0.7161 - val_loss: 1.0913 - val_accuracy: 0.5992\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 162s 362ms/step - loss: 0.7517 - accuracy: 0.7213 - val_loss: 1.0724 - val_accuracy: 0.6097\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 162s 362ms/step - loss: 0.7331 - accuracy: 0.7341 - val_loss: 1.0706 - val_accuracy: 0.6147\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 162s 362ms/step - loss: 0.7053 - accuracy: 0.7429 - val_loss: 1.0860 - val_accuracy: 0.6077\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.6877 - accuracy: 0.7483 - val_loss: 1.0717 - val_accuracy: 0.6123\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 163s 364ms/step - loss: 0.6604 - accuracy: 0.7604 - val_loss: 1.0933 - val_accuracy: 0.6062\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 163s 364ms/step - loss: 0.6433 - accuracy: 0.7661 - val_loss: 1.1039 - val_accuracy: 0.6122\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 163s 365ms/step - loss: 0.6203 - accuracy: 0.7739 - val_loss: 1.1075 - val_accuracy: 0.6165\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.5980 - accuracy: 0.7831 - val_loss: 1.1034 - val_accuracy: 0.6138\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 164s 365ms/step - loss: 0.5780 - accuracy: 0.7915 - val_loss: 1.1018 - val_accuracy: 0.6145\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 164s 365ms/step - loss: 0.5638 - accuracy: 0.7961 - val_loss: 1.1125 - val_accuracy: 0.6123\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 163s 364ms/step - loss: 0.5418 - accuracy: 0.8023 - val_loss: 1.1306 - val_accuracy: 0.6076\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.5244 - accuracy: 0.8090 - val_loss: 1.1265 - val_accuracy: 0.6124\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 163s 364ms/step - loss: 0.5004 - accuracy: 0.8202 - val_loss: 1.1429 - val_accuracy: 0.6101\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.4870 - accuracy: 0.8241 - val_loss: 1.1549 - val_accuracy: 0.6154\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 162s 362ms/step - loss: 0.4721 - accuracy: 0.8309 - val_loss: 1.1581 - val_accuracy: 0.6159\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.4620 - accuracy: 0.8354 - val_loss: 1.1528 - val_accuracy: 0.6169\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 162s 363ms/step - loss: 0.4426 - accuracy: 0.8398 - val_loss: 1.1563 - val_accuracy: 0.6204\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.4286 - accuracy: 0.8454 - val_loss: 1.1774 - val_accuracy: 0.6184\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 163s 364ms/step - loss: 0.4097 - accuracy: 0.8544 - val_loss: 1.1947 - val_accuracy: 0.6176\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 164s 366ms/step - loss: 0.4040 - accuracy: 0.8554 - val_loss: 1.1835 - val_accuracy: 0.6186\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 165s 368ms/step - loss: 0.3815 - accuracy: 0.8627 - val_loss: 1.2167 - val_accuracy: 0.6207\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 163s 363ms/step - loss: 0.3826 - accuracy: 0.8620 - val_loss: 1.2171 - val_accuracy: 0.6152\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 164s 366ms/step - loss: 0.3686 - accuracy: 0.8694 - val_loss: 1.2405 - val_accuracy: 0.6166\n"
     ]
    }
   ],
   "source": [
    "#Trainging the neural network\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch=28709//64,\n",
    "epochs=50,\n",
    "validation_data=val_generator,\n",
    "validation_steps= 7178//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b0febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\",'w')as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "#saving trained model weight\n",
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92a101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
